{% extends "springeval/base.html" %}

{% block content %}

<h3>Frequently Asked Questions</h3>


<h4>How does the registration work?</h4>
<p>In order to submit to the benchmark, i.e. evaluate results on the Spring <i>test</i> split, you need to register.
Please note that in order to <a href="https://doi.org/10.18419/darus-3376" target="_blank" rel="noopener noreferrer">download</a> the dataset, no registration is needed.
To register, first <a href="{% url 'springeval:signup' %}">create an account</a> and confirm your mail address by clicking on the link we will send to you.
Afterwards, your account request will be verified by our team. Finally, after successful verification, you can submit results for evaluation.
</p>


<h4>How does the submission work?</h4>
<p>
To participate in the benchmark, follow the steps below to prepare and submit
your method. You can optionally include robustness evaluation results alongside
the standard evaluation.
</p>
<ol>
  <li>
    <strong>Prepare predictions on the Spring test split.</strong>
    <ul>
      <li>
        You can find Python code to read/write <code>.flo5</code> and
        <code>.dsp5</code> files
        <a
          href="https://github.com/cv-stuttgart/flow_library/blob/master/flow_IO.py"
          target="_blank"
          rel="noopener noreferrer"
          >here</a
        >.
      </li>
      <li>
        Please follow the exact folder structure, file naming conventions, and
        formats used in the dataset.
      </li>
      <li><strong>Standard evaluation folder structure:</strong></li>
      <ul>
        <li>
          Stereo:
          <code
            >&lt;rootdir&gt;/####/disp1_{left|right}/disp1_{left|right}_####.dsp5</code
          >
        </li>
        <li>
          Optical Flow:
          <code
            >&lt;rootdir&gt;/####/flow_{FW|BW}_{left|right}/flow_{FW|BW}_{left|right}_####.flo5</code
          >
        </li>
        <li>
          Scene Flow: Add in disparity over time:
          <code
            >&lt;rootdir&gt;/####/disp2_{FW|BW}_{left|right}/disp2_{FW|BW}_{left|right}_####.dsp5</code
          >
        </li>
      </ul>
      <li><strong>Optional: Robustness evaluation</strong></li>
      <ul>
        <li>
          You can optionally submit predictions on corrupted versions of the test set.
        </li>
        <li>
          Use the same folder structure and naming as above, but with an
          additional top-level folder for the corruption type.
        </li>
        <li><strong>Robustness evaluation folder structure:</strong></li>
        <ul>
          <li>
            Stereo:
            <code
              >&lt;rootdir&gt;/&lt;corruption&gt;/test/####/disp1_{left|right}/disp1_{left|right}_####.dsp5</code
            >
          </li>
          <li>
            Optical Flow:
            <code
              >&lt;rootdir&gt;/&lt;corruption&gt;/test/####/flow_{FW|BW}_{left|right}/flow_{FW|BW}_{left|right}_####.flo5</code
            >
          </li>
          <li>
            Scene Flow:
            <code
              >&lt;rootdir&gt;/&lt;corruption&gt;/test/####/disp2_{FW|BW}_{left|right}/disp2_{FW|BW}_{left|right}_####.dsp5</code
            >
          </li>
        </ul>
        <li>
          The corruption folders should be named as follows: <code>clean</code>, <code>brightness</code>, <code>contrast</code>, <code>defocus_blur</code>, <code>elastic_transform</code>, <code>fog</code>, <code>frost</code>, <code>gaussian_blur</code>, <code>gaussian_noise</code>, <code>glass_blur</code>, <code>impulse_noise</code>, <code>jpeg_compression</code>, <code>motion_blur</code>, <code>pixelate</code>, <code>rain</code>, <code>saturate</code>, <code>shot_noise</code>, <code>snow</code>, <code>spatter</code>, <code>speckle_noise</code>, <code>zoom_blur</code>.
        </li>
      </ul>
    </ul>
  </li>
  <li>
    <strong>Generate submission file(s) using the subsampling tool.</strong>
    <ul>
      <li>
        Download the subsampling executables from
        <a
          href="https://cloud.visus.uni-stuttgart.de/index.php/s/RSgYPXdKTyAFfWy"
          target="_blank"
          rel="noopener noreferrer"
          >this link</a
        >.
      </li>
      <li>Run the appropriate tool in your root directory:</li>
      <ul>
        <li><code>./disp1_subsampling &lt;rootdir&gt;</code> for stereo</li>
        <li>
          <code>./flow_subsampling &lt;rootdir&gt;</code> for optical flow
        </li>
        <li>
          <code>./disp2_subsampling &lt;rootdir&gt;</code> for scene flow
          (alongside the other two)
        </li>
      </ul>
      <li>
        <strong>For robustness evaluation</strong>, run the corresponding
        “robust” executables in your root directory containing the corruption subfolders:</li>
      <ul>
        <li>
          <code>./disp1_robust_subsampling &lt;rootdir&gt;</code>
          for stereo robustness
        </li>
        <li>
          <code>./flow_robust_subsampling &lt;rootdir&gt;</code>
          for optical flow robustness
        </li>
        <li>
          <code>./disp2_robust_subsampling &lt;rootdir&gt;</code>
          for scene flow robustness (alongside the other two)
        </li>
      </ul>
      <li>
        Each command generates one <code>.hdf5</code> file in the
        current directory, ready for upload.
      </li>
    </ul>
  </li>
  <li>
    <strong>Submit your results on the benchmark platform.</strong>
    <ul>
      <li>
        Upload your <code>.hdf5</code> file(s) via the
        <a href="{% url 'springeval:user' %}">submission page</a>.
      </li>
      <li>
        For scene flow, all three <code>.hdf5</code> files must be uploaded.
      </li>
      <li>
        You can optionally upload additional files for robustness evaluation
        under the same method name.
      </li>
      <li>
        Evaluation may take up to 1-2 hours.
      </li>
      <li>
        All results are private by default. You can later change visibility to:
        <ul>
          <li><strong>Private</strong> (only visible to you)</li>
          <li>
            <strong>Public Anonymous</strong> (visible on the leaderboard
            without author info)
          </li>
          <li><strong>Public</strong> (visible with author and method name)</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>


<h4>Which data formats do you provide?</h4>
<p>Images and maps are given in <code>png</code> format. Disparity and optical flow files are given in <a href="https://www.hdfgroup.org/" target="_blank" rel="noopener noreferrer">HDF5</a> file format and named <code>.dsp5</code> for disparity and <code>.flo5</code> for optical flow.
You can find reference code to read/write <code>.flo5</code> and <code>.dsp5</code> files <a href="https://github.com/cv-stuttgart/flow_library/blob/master/flow_IO.py" target="_blank" rel="noopener noreferrer">here</a>.
</p>


<h4>Do you also provide point clouds for the Spring dataset?</h4>
<p>You can find code to transform Spring data to point 3D point clouds <a href="https://github.com/cv-stuttgart/spring_utils" target="_blank" rel="noopener noreferrer">here</a>.</p>


<h4>How can I compute metric depth from disparity?</h4>
<p>In general, depth <code>Z</code> is computed from disparity <code>d</code> through <code>Z = fx * B / d</code>, where <code>fx</code> is the focal length in pixels (given in intrinsics.txt) and <code>B</code> is the stereo camera baseline distance; for Spring this is always 0.065m. Please note that the Spring dataset encodes infinitely distant sky pixels as zero disparity, leading to infinite values when using the above formula.</p>


<h4>Which vector length is encoded in the ground truth data?</h4>
<p>While our ground truth files are given in 4K (double the spatial resolution per dimension), the ground truth vectors (disparities, optical flow) relate to the <i>images in full HD resolution</i>. So when using these, there should be no need to divide them by 2. See also the example data loader for Spring <a href="https://github.com/cv-stuttgart/spring_utils" target="_blank" rel="noopener noreferrer">here</a>.</p>


<h4>Which (stereo) camera settings does Spring use?</h4>
<p>Spring uses an orthoparallel stereo camera setting, i.e. two cameras parallely pointing into the same direction. The baseline distance between the cameras is 0.065m. Intrinsic camera parameters (available for <i>train</i> and <i>test</i>) are given per sequence in intrinsics.txt, extrinsic camera data / camera poses (available for <i>train</i>) are given in extrinsics.txt. Please note that in some scenes a camera zoom / change of the focal length is used, leading to different intrinsics per frame.
We additionally provide metric camera focal distances in focaldistance.txt.
</p>

<h4>Are there nan values in the ground truth files?</h4>
<p>Yes, very occasionally, there are nan values in the ground truth files, which arise from a bug in the Blender/cycles shading system. During evaluation, these pixels are ignored.</p>


<h4>I have further questions!</h4>
<p>Please use the contact link at the bottom of the page.</p>

{% endblock %}
